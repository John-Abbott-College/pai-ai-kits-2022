# Bias in AI

### Defining Bias

> “Bias is a prejudice for or against something or somebody, that may result in unfair decisions.” (European Commission, 2018)

<details>

<summary>Learn More</summary>

Humans are known to exhibit bias in their decisions, and since AI systems are created by humans, there is a possibility of unintentionally injecting bias into them (European Commission, 2018). This is especially true in AI systems that rely on machine learning techniques and the collection and selection of training data (European Commission, 2018).&#x20;

If the training data is not diverse and balanced, the system may learn to make unfair decisions (European Commission, 2018). When AI systems are applied to scenarios involving people, biases related to shapes, colors, skin color, or gender can arise ([Ref](https://www.digitaltechnologieshub.edu.au/teach-and-assess/classroom-resources/lesson-ideas/data-bias-in-ai/)). &#x20;

These biases present ethical concerns as AI systems can reinforce and amplify existing biases, and their decision-making process may be difficult to understand or question due to their complex nature (Shaw, 2019). Despite the superior processing capabilities of AI, it cannot be assumed to always be fair and unbiased since it is created by fallible humans prone to bias and judgment (Bossman, 2016).&#x20;

</details>

### Uncovering Bias in AI

_Gender Shades_, a groundbreaking study conducted by MIT researcher Joy Buolamwini, uncovered bias in facial recognition algorithms.

Select the button below to play _Gender Shades_.&#x20;

{% embed url="https://www.youtube.com/watch?embeds_referring_euri=http://gendershades.org/&feature=emb_logo&source_ve_path=Mjg2NjY&v=TWWsW1w-BVo" %}

<details>

<summary><mark style="color:purple;"><strong>Debrief</strong></mark></summary>

<mark style="color:purple;">Take a moment to reflect and respond to the following questions. Share your responses</mark> [<mark style="color:blue;background-color:blue;">**here**</mark>](https://jamboard.google.com/d/1RZ2njer7ZINALAPXLvb2Holscbk0MVI4NOlPH0USiAY/edit?usp=sharing)<mark style="color:purple;">**.**</mark>&#x20;

* <mark style="color:purple;">Reflect on the potential consequences of relying heavily on facial recognition systems without considering their limitations and biases.</mark> &#x20;
* <mark style="color:purple;">What ethical implications arise from the use of facial recognition technology in various domains such as law enforcement, hiring processes, and public surveillance?</mark> &#x20;

</details>

<details>

<summary>Learn More</summary>

Commonly used facial recognition systems have been shown to over-represent certain populations, leading to better performance for them while neglecting others ([Ref](https://docs.google.com/document/d/1i\_\_XQcSVF1BfHCFWRZ3GkLaqWde0RVxyz2o85xBMMJw/edit)). This imbalance in training data, known as "selection bias," particularly affects dark-skinned women ([Ref](https://docs.google.com/document/d/1i\_\_XQcSVF1BfHCFWRZ3GkLaqWde0RVxyz2o85xBMMJw/edit)).&#x20;

Unfortunately, little progress has been made in accurately recognizing facial features of black women since Buolamwini's investigation in 2015 (Babusi, 2020). Recent studies have shown that general application facial recognition systems in the US misidentify people of color at significantly higher rates than white individuals (Babusi, 2020). &#x20;

</details>
