# Evaluating Your Model \[Activity]

Now that you've built your model, let's put it to the test!&#x20;

Testing AI is not just about how accurate its predictions are. Anyone involved in the development or deployment of AI technologies must be mindful of the impact they can have. As we learned earlier, AI technologies have the potential to cause harm. To avoid doing harm, AI must be developed and deployed with ethical principles in mind.&#x20;

## Principles for Ethical AI

While there is no universal set of guidelines or regulations governing AI, a consensus is beginning to emerge around the importance of the following principles: **responsibility, fairness, transparency, and freedom** (Jobin et al., 2019).&#x20;

<table data-header-hidden><thead><tr><th width="173">Principle</th><th>Description</th></tr></thead><tbody><tr><td><strong>Responsibility</strong></td><td>Encompasses the accountability and conscientiousness required from individuals and organizations involved in the development, deployment, and use of artificial intelligence systems. </td></tr><tr><td><strong>Fairness</strong></td><td>Encompasses the pursuit of equitable and impartial treatment of individuals and groups throughout the development, deployment, and utilization of AI systems.</td></tr><tr><td><strong>Transparency</strong></td><td>Refers to the degree to which AI systems' inner workings, decision-making processes, and underlying data are open, understandable, and accessible to users, stakeholders, and the general public.</td></tr><tr><td><strong>Freedom</strong></td><td>Refers to the protection and preservation of individual autonomy, privacy, and agency in the face of AI systems.</td></tr></tbody></table>

<details>

<summary>Learn More - AI Ethical Principle Details &#x26; Challenges With Making AI More Ethical</summary>

### AI Ethical Principle Details

#### Responsibility &#x20;

* Responsibility means recognizing and accepting responsibility for the actions and consequences arising from the use of AI systems. This involves being answerable for the decisions made, the data used, and the outcomes produced by AI technologies.&#x20;
* There is ongoing debate surrounding ethical principles in AI due to the absence of definitive guidelines, with particular emphasis on the contentious issue of assigning responsibility for the outcomes of AI. &#x20;
* Various stakeholders have been identified as responsible and liable for the actions and choices of AI, including AI developers, designers, institutions, and the industry (Jobin et al., 2019). Additionally, there is disagreement regarding whether AI should be held accountable in a manner resembling human accountability, or if humans should always bear the ultimate responsibility for technological artifacts (Jobin et al., 2019). &#x20;

#### Fairness &#x20;

* To ensure fairness in AI systems, it is crucial to address biases inherent in both the data employed for training AI systems and the systems themselves. &#x20;
* By mitigating these biases, it is possible to prevent AI applications from yielding unjust or discriminatory consequences that hinge on factors like race, gender, ethnicity, or socioeconomic status.&#x20;

#### Transparency &#x20;

* Transparency, in the context of AI ethics, refers to the degree to which AI systems' inner workings, decision-making processes, and underlying data are open, understandable, and accessible to users, stakeholders, and the general public. &#x20;
  * It involves ensuring that AI systems are not overly opaque or "black-boxed," but instead promote clarity and accountability by providing explanations, justifications, and insights into how they operate, how they arrive at decisions, and how they handle and process data. &#x20;
* Efforts to enhance transparency involve endeavors aimed at improving explainability, auditability, or other forms of communication and disclosure (Jobin et al., 2019) &#x20;
  * **Explainability** pertains to the assurance that decisions made by AI systems, along with the data influencing those decisions, can be clarified to end-users and other relevant parties using language that is not overly technical. (Fat/ML) &#x20;
  * **Auditability** encompasses the provision of opportunities for external third parties to comprehensively understand, thoroughly investigate, and critically assess AI systems. Auditability can be supported through permissive terms of use, detailed documentation, and disclosure of information that enables monitoring, checking, and criticism. (Fat/ML) &#x20;

#### Freedom &#x20;

* To properly safeguard freedom in the face of AI systems, it is essential to ensure that AI technologies do not unduly limit or infringe upon the fundamental rights and freedoms of individuals.&#x20;
* To be free, individuals must have:&#x20;
  * Ability to make informed choices&#x20;
  * Control over their personal data&#x20;
  * Protection against unwarranted surveillance&#x20;
  * Protection against manipulation by AI systems&#x20;

### Challenges With Making AI More Ethical

By acknowledging the potential biases and ethical considerations associated with AI, we can strive to develop AI systems that are fair, accountable, and uphold ethical standards, while promoting inclusivity. However, achieving these goals can be challenging due to the **complexity of AI systems**, which often involve numerous variables that make comprehensive understanding difficult. &#x20;

It is important to note that there is no one set of rules that have been agreed upon to guide the development and deployment of ethical AI. **Ethical questions in AI are still subject to ongoing debates and discussions that are expected to persist for years to come** (Borenstein et al., 2021). Despite these challenges, it is essential to make continuous efforts in creating more ethical AI systems.&#x20;

</details>

## Evaluate Your Model&#x20;

### Activity Overview

#### Aim

* Consider the ethical implications of using your model in a real-life scenario&#x20;
* Think of ways to mitigate any potential harm your model could cause&#x20;

#### Instructions&#x20;

1. Split into groups (4-6 people)&#x20;
2. Assign the following roles within your group: **leader, notetaker, timekeeper,** and **presenter**&#x20;
3. Read the scenario&#x20;
4. Reflect on the accuracy of your model
5. Evaluate your model with respect to the scenario and the ethical principle (**responsibility, fairness, transparency,** or **freedom**) your teacher assigned to your group

## **Scenario**&#x20;

Your model has gained widespread attention due to its potential to revolutionize the **delivery industry** by significantly reducing the number of accidents caused by driver negligence or fatigue. Delivery companies across the country are eager to adopt your technology, believing it will enhance road safety and improve the efficiency of their operations.

To help you imagine this scenario, review the video below that promotes a real-life DDD system - **Driveri** which is currently being used by Amazon to monitor their delivery drivers.&#x20;

{% embed url="https://www.youtube.com/watch?v=W2F-3oq7nDI" %}

## Evaluation

Consider the broader implications of putting your model on the market. Reflect on the ethical implications involved in deploying such a technology and how ethical principles can be upheld while the benefits of your innovation are maximized.

### Accuracy

Reflect on the accuracy of the model you created. Write your answers to the following questions [here](https://jamboard.google.com/d/1FlTpJDsXlAhTQZpGRK3j6IXyUsO0nTzYn1gplZQW-18/edit?usp=sharing).&#x20;

* <mark style="color:purple;">Were there instances where your model made accurate predictions. Why do you think it worked?</mark>
* <mark style="color:purple;">Were there instances where your model made inaccurate predictions. Why do you think it did not work?</mark>

### Ethics

Answer the questions related to the ethical principle you have been assigned. &#x20;

<table data-card-size="large" data-view="cards"><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td><strong>Responsibility</strong></td><td><mark style="color:purple;">What level of responsibility do you believe technology developers have in ensuring the ethical implications and consequences of their creations? Should there be a code of ethics specific to driver monitoring models? If so, what principles should it encompass?</mark></td><td><mark style="color:purple;">While the model aims to reduce accidents caused by human error, what happens when the model itself malfunctions or makes incorrect judgments? How can you address the possibility of false positives or false negatives that may lead to unfair penalties or compromised safety?</mark></td></tr><tr><td><strong>Fairness</strong></td><td><mark style="color:purple;">How can you ensure that the model does not disproportionately affect certain individuals or groups, such as based on race, gender, or socioeconomic background? What steps can be taken to mitigate bias and discrimination in the model's decision-making process?</mark></td><td><mark style="color:purple;">Should drivers have the ability to contest or appeal the decisions made by the driver monitoring model? How can you strike a balance between providing drivers with agency and upholding safety standards?</mark></td></tr><tr><td><strong>Transparency</strong></td><td><mark style="color:purple;">How can you make the decision-making process of the model transparent to both drivers and the general public? What mechanisms or explanations can be provided to ensure that the system's judgments are understandable and explainable?</mark></td><td><mark style="color:purple;">How can you ensure transparency in how the data collected by the model is utilized? What measures can be implemented to inform drivers about the types of data being collected, how it will be used, and how long it will be retained?</mark></td></tr><tr><td><strong>Freedom</strong></td><td><mark style="color:purple;">How can the driver monitoring model strike a balance between enhancing safety and preserving the freedom and autonomy of drivers? Should drivers have the ability to disable or override the monitoring system in certain situations?</mark></td><td><mark style="color:purple;">Does the implementation of driver monitoring models raise concerns about freedom of movement and privacy, as it restricts or monitors drivers' actions? How can you address these concerns while ensuring road safety?</mark></td></tr></tbody></table>

## Present

Add your responses [here](https://jamboard.google.com/d/13uDPBM8rb5NPygaDGmgjRU5\_WsCCSKAdv1gFqhgRP7k/edit?usp=sharing) and be ready to share them with your class.

<details>

<summary>Learn More - DDD System Accuracy Issues &#x26; Potential Solutions</summary>

DDD systems encounter numerous challenges that limit their accuracy.

### Common Accuracy Issues&#x20;

* **Lack of variation in training classes** can reduce accuracy, especially for individuals with darker skin colors
* **Overfitting**&#x20;
  * occurs when models memorize noise instead of generalizable features, leading to biased predictions
  * can lead to biased predictions and perpetuate discriminatory outcomes
* More diverse training data is needed to address these issues, but **AI systems still need to be monitored**

### **Potential Causes of Accuracy Issues**&#x20;

DDD systems encounter accuracy challenges due to several factors. One reason is the limited variation in frame sequences for the classes of interest, such as looking forward, looking left, or looking right (Chai et al., 2023). Unlike different human activities like jumping or squatting, the variation in head movements is minimal, while other features, such as appearance and clothing, vary significantly across different drivers (Chai et al., 2023). This can lead to overfitting and poor generalization when using convolution-based models trained and tested on different drivers (Chai et al., 2023). &#x20;

Additionally, unlike traditional human activity classification where multiple frames contribute to determining the action label, in driver monitoring, the label primarily depends on frames towards the end, such as distinguishing between looking left and looking forward (Chai et al., 2023). Moreover, the camera's position can pose an additional challenge by decreasing the visual differences between different classes, such as looking forward and looking left (Chai et al., 2023).  DDD systems face accuracy challenges due to various factors. One such factor is the limited variation in frame sequences for the classes of interest. For DDD systems that detect distraction based on the driver’s head position, the main classes of interest would include actions such as turning one’s head to the left, right, or downwards. The system could be trained to associate these classes with various levels of distraction. For example, if the driver’s head is turned downwards, they could be looking at their phone and so, this action would be classified as driving distraction.&#x20;

However, unlike different human activities like jumping or squatting, the variation in head movements is minimal, while other features, such as appearance and clothing, vary significantly across different drivers (Chai et al., 2023). A lack of variation in the training classes for DAD systems can make its predictions less accurate, especially for people of darker skin colors (Albadawi, Takruri, & Award, 2022). This is an example of overfitting, which is a concerning machine learning behaviour that can lead to discrimination as it makes the model biased toward certain groups over others.&#x20;

When a model is trained on a small range of frame sequences, it may be more susceptible to overfitting. Overfitting occurs when the model memorizes specific patterns or noise in the training data instead of learning the underlying generalizable features. This can result in the model making biased predictions based on irrelevant or spurious correlations present in the limited data, reinforcing discriminatory biases and perpetuating unfair outcomes.&#x20;

### Translating DAD Systems from Simulations to Real-Life

DAD systems are typically developed and tested in simulated environments, posing significant challenges when translating them for real-life applications (Albadawi, Takruri, & Award, 2022).&#x20;

<img src="../.gitbook/assets/driving simulator (from Ahangari et al., 2020.png" alt="Driving Simulator" data-size="original"><img src="../.gitbook/assets/distracted-driving netradyne.webp" alt="Netradyne Driver Simulator " data-size="original">

While simulated environments allow for control over intervening variables, these variables can reduce a system’s accuracy when it is put to work in the real world. Some **intervening variables** that can interfere with a system’s performance include:&#x20;

* Illumination variation (Addanki et al., 2020; Albadawi, Takuri, & Award, 2022)
* Background variation (Albadawi, Takruri, & Award, 2022)
* Movements such as swallowing, blinking, or touching one’s face ([Ref](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9482962/))&#x20;
* Features that cover the mouth or eyes such as a beard, mustache, and sunglasses (Albadawi, Takruri, & Award, 2022)&#x20;

Given the impact of these variables, recalibration of the system becomes essential when a new driver assumes control. Neglecting to recalibrate the system can introduce biases and lead to inaccurate indications of distracted driving.&#x20;

### Issues with Amazon's Driveri&#x20;

Concerns have been raised regarding **privacy**, with drivers describing the cameras as **intrusive** and akin to a **punishment** system.&#x20;

The use of AI monitoring raises accuracy and fairness concerns, as AI struggles to comprehend human behavior nuances and anomalies, potentially leading to **errors in judgment**.&#x20;

Furthermore, the refined AI capabilities may exert increasing pressure on drivers to conform their behavior, movements, and appearance to satisfy the demands of the **surveillance** AI system. For instance, if caught yawning, drivers are instructed to pull over for a minimum of 15 minutes, with non-compliance possibly resulting in a call from their supervisor.&#x20;

### Potential Solutions&#x20;

To address some of these challenges, it is essential to collect more diverse data. If the training data does not adequately represent the diversity of real-world driving scenarios and driver behaviors, the model may struggle to make accurate predictions for underrepresented groups or specific situations. If the training data predominantly consists of drivers of a specific gender, age group, or ethnicity, the model may not generalize well to diverse populations and exhibit biased predictions. This can lead to discriminatory outcomes, where certain groups are unfairly targeted or disadvantaged.

However, it is important to note, that the training phase machine learning systems undergo cannot encompass all possible real-world examples (Bossman, 2016). These systems can be vulnerable to deception in ways that humans wouldn't be (Bossman, 2016). To ensure the desired performance and prevent misuse, it is crucial to carefully monitor and safeguard AI systems as they become integral to various aspects of our lives, such as labor, security, and efficiency (Bossman, 2016)

</details>

## Improve your model&#x20;

There are several approaches researchers are taking to mitigate or minimize bias in AI systems, such as: &#x20;

* utilizing feedback and learning from errors by testing AI systems with a diverse range of individuals ([Ref](https://docs.google.com/document/d/1i\_\_XQcSVF1BfHCFWRZ3GkLaqWde0RVxyz2o85xBMMJw/edit))
* ensuring a balance in the training data employed to develop AI models ([Ref](https://docs.google.com/document/d/1i\_\_XQcSVF1BfHCFWRZ3GkLaqWde0RVxyz2o85xBMMJw/edit))&#x20;
* developing novel methods to detect and identify biases in AI systems ([Ref](https://docs.google.com/document/d/1i\_\_XQcSVF1BfHCFWRZ3GkLaqWde0RVxyz2o85xBMMJw/edit))
* establishing open datasets and fostering collaborations through partnerships ([Ref](https://docs.google.com/document/d/1i\_\_XQcSVF1BfHCFWRZ3GkLaqWde0RVxyz2o85xBMMJw/edit))

Think about how you can improve your model.&#x20;
